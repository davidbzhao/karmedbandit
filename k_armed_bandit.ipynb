{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k_armed_bandit.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "U1sREqKkp-33",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The k-armed bandit problem\n",
        "In reinforcement learning, there is the classic k-armed bandit problem. In this problem, there are K possible actions for an agent to take, each action having some value associated with it that is unknown to the agent. It is the agent's goal to maximize value gained over a defined T number of time steps. Because the agent starts with no knowledge of the values, it must explore its options to ensure, when it exploits its knowledge, it can do so with some reasoning."
      ]
    },
    {
      "metadata": {
        "id": "cK_lKRIcoy73",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from random import normalvariate, randint, random, seed\n",
        "seed(58)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJsHXqq63ydN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Action Space\n",
        "The action space defines what actions an agent in this environment can take. In this case, there are K actions in an action space, with the value of each action being defined by a normal distribution."
      ]
    },
    {
      "metadata": {
        "id": "XGOG6veqs68X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ActionSpace:\n",
        "  def __init__(self, K, mu_range=[1, 10], sd_max=1):\n",
        "    self.K = K\n",
        "    self.dists = [(randint(mu_range[0], mu_range[1]), random() * sd_max) for _ in range(K)]\n",
        "  \n",
        "  def __str__(self):\n",
        "    format_str = self.__repr__()\n",
        "    for k in range(self.K):\n",
        "      format_str += '\\n\\tQ[{}] = N(mu={}, sigma={:.3f})'.format(k, self.dists[k][0], self.dists[k][1])\n",
        "    return format_str\n",
        "  \n",
        "  def get_dists(self):\n",
        "    return self.dists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UdNwPAIm3_HG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "Each agent has a model of the values of the actions it takes. In this case, the model defines the value of an action as the average of past values."
      ]
    },
    {
      "metadata": {
        "id": "gaX8ZR77043E",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "  def __init__(self, K):\n",
        "    assert K > 0, 'K must be at least 1'\n",
        "    \n",
        "    self.K = K\n",
        "    self.values = [[] for _ in range(K)]\n",
        "    \n",
        "  def __str__(self):\n",
        "    format_str = self.__repr__()\n",
        "    for k in range(self.K):\n",
        "      format_str += '\\n\\tA[{}] = {}'.format(k, self.values[k])\n",
        "    return format_str\n",
        "      \n",
        "  def add_value(self, action, value):\n",
        "    assert action >= 0 and action < self.K, 'Action must be in range [0, {})'.format(self.K)\n",
        "    self.values[action].append(value)\n",
        "      \n",
        "  def get_value(self, action):\n",
        "    assert action >= 0 and action < self.K, 'Action must be in range [0, {})'.format(self.K)\n",
        "    values = self.values[action]\n",
        "    num_values = len(values)\n",
        "    if num_values == 0:\n",
        "      return 0\n",
        "    return sum(values) / num_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jvrLU-c54WO2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Agent\n",
        "An agent has a few properties. An agent must track the value its received thus far, the number of actions taken, and a model of action values. The epsilon value of an agent defines how likely it is to explore on any given step. In this case, exploration entails selecting an action at random, regardless of previous knowledge."
      ]
    },
    {
      "metadata": {
        "id": "nNuxCt0Gp2FL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  \n",
        "  def __init__(self, K, eps):\n",
        "    assert eps >= 0 and eps <= 1, 'Epsilon must be in the range [0, 1]'\n",
        "    assert K > 0, 'K must be at least 1'\n",
        "    assert isinstance(K, int), 'K must be an integer'\n",
        "    \n",
        "    self.eps = eps\n",
        "    self.value = 0\n",
        "    self.steps = 0\n",
        "    self.K = K\n",
        "    self.model = Model(K)\n",
        "    \n",
        "  def __str__(self):\n",
        "    return '{}\\n\\tvalue={},\\n\\teps={},\\n\\tsteps={}\\n\\tK={}\\n\\tmodel={}'.format(\n",
        "      self.__repr__(), self.value, self.eps, self.steps, self.K, self.model.__str__())\n",
        "  \n",
        "  def get_value(self):\n",
        "    return self.value\n",
        "  \n",
        "  def take_action(self, action_space):\n",
        "    dists = action_space.get_dists()\n",
        "    assert len(dists) >= self.K, 'Agent cannot know more actions than exist'\n",
        "    \n",
        "    # Select action to take\n",
        "    if random() < self.eps:  # random choice\n",
        "      action = randint(0, self.K - 1)\n",
        "    else:  # greedy choice\n",
        "      max_value = self.model.get_value(0)\n",
        "      max_value_action = 0\n",
        "      for i in range(1, self.K):\n",
        "        value_i = self.model.get_value(i)\n",
        "        if value_i > max_value:\n",
        "          max_value = value_i\n",
        "          max_value_action = i\n",
        "      action = max_value_action\n",
        "      \n",
        "    # Get value of action\n",
        "    mu, sigma = dists[action]\n",
        "    value = normalvariate(mu, sigma)\n",
        "    \n",
        "    # Update self\n",
        "    self.value += value\n",
        "    self.model.add_value(action, value)\n",
        "    self.steps += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "21uTzSNe46wX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiment\n",
        "Let's initialize an action space and a few different agents. One agent will always take the greedy choice, another agent will make uniformly random choices, and the final agent will attempt to balance greedy and exploratory actions."
      ]
    },
    {
      "metadata": {
        "id": "OGi0lkphsdvG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "K = 5\n",
        "action_space = ActionSpace(K=K, mu_range=[1, 10], sd_max=2)\n",
        "agent = Agent(K=K, eps=0.5)\n",
        "greedy_agent = Agent(K=K, eps=0)\n",
        "random_agent = Agent(K=K, eps=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZWnm1EQsv9K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "141c880f-b61c-4173-d417-a3bd8db7cb49",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528052375795,
          "user_tz": 240,
          "elapsed": 463,
          "user": {
            "displayName": "David Zhao",
            "photoUrl": "//lh4.googleusercontent.com/-u2kDqJjOLoc/AAAAAAAAAAI/AAAAAAAAc_o/eqcCIXDOBuM/s50-c-k-no/photo.jpg",
            "userId": "113711053352969780071"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "T = 25\n",
        "for _ in range(T):\n",
        "  agent.take_action(action_space)\n",
        "  greedy_agent.take_action(action_space)\n",
        "  random_agent.take_action(action_space)\n",
        "print(action_space)\n",
        "print('normal: Agent(value={})'.format(agent.get_value()))\n",
        "print('greedy: Agent(value={})'.format(greedy_agent.get_value()))\n",
        "print('random: Agent(value={})'.format(random_agent.get_value()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.ActionSpace object at 0x7f70d5c90cc0>\n",
            "\tQ[0] = N(mu=10, sigma=0.394)\n",
            "\tQ[1] = N(mu=4, sigma=0.082)\n",
            "\tQ[2] = N(mu=8, sigma=0.811)\n",
            "\tQ[3] = N(mu=5, sigma=1.611)\n",
            "\tQ[4] = N(mu=5, sigma=1.933)\n",
            "normal: Agent(value=191.86696244083606)\n",
            "greedy: Agent(value=248.29656530594724)\n",
            "random: Agent(value=149.70856842929214)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aiAuwRKy50rg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that the greedy agent outperforms the other two agents in this case. But it looks like the greedy agent just got lucky with picking the best action first. What if we try it again?"
      ]
    },
    {
      "metadata": {
        "id": "kM0bW4gf7Ysj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "K = 5\n",
        "action_space = ActionSpace(K=K, mu_range=[1, 10], sd_max=2)\n",
        "agent = Agent(K=K, eps=0.5)\n",
        "greedy_agent = Agent(K=K, eps=0)\n",
        "random_agent = Agent(K=K, eps=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q__SeY8X7Z0J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "d4c43a34-e95f-415b-df9a-49532385cfa3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528052376611,
          "user_tz": 240,
          "elapsed": 299,
          "user": {
            "displayName": "David Zhao",
            "photoUrl": "//lh4.googleusercontent.com/-u2kDqJjOLoc/AAAAAAAAAAI/AAAAAAAAc_o/eqcCIXDOBuM/s50-c-k-no/photo.jpg",
            "userId": "113711053352969780071"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "T = 25\n",
        "for _ in range(T):\n",
        "  agent.take_action(action_space)\n",
        "  greedy_agent.take_action(action_space)\n",
        "  random_agent.take_action(action_space)\n",
        "print(action_space)\n",
        "print('normal: Agent(value={})'.format(agent.get_value()))\n",
        "print('greedy: Agent(value={})'.format(greedy_agent.get_value()))\n",
        "print('random: Agent(value={})'.format(random_agent.get_value()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.ActionSpace object at 0x7f70d64f68d0>\n",
            "\tQ[0] = N(mu=5, sigma=1.241)\n",
            "\tQ[1] = N(mu=3, sigma=0.905)\n",
            "\tQ[2] = N(mu=8, sigma=1.435)\n",
            "\tQ[3] = N(mu=7, sigma=1.269)\n",
            "\tQ[4] = N(mu=8, sigma=1.735)\n",
            "normal: Agent(value=190.04331672224)\n",
            "greedy: Agent(value=122.19357756113926)\n",
            "random: Agent(value=162.65352720701765)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6x2tUbDR7iTG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we see that the agent that balances exploration and exploitation outperforms the other two. The greedy agent, in this case, got unlucky, picking one of the worst actions first and then sticking with it.\n",
        "\n",
        "Let's try running our experiment 100 times."
      ]
    },
    {
      "metadata": {
        "id": "b1bn617A2-8j",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ba4f14d9-3c13-40d5-8721-486164dd3778",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528052378929,
          "user_tz": 240,
          "elapsed": 2208,
          "user": {
            "displayName": "David Zhao",
            "photoUrl": "//lh4.googleusercontent.com/-u2kDqJjOLoc/AAAAAAAAAAI/AAAAAAAAc_o/eqcCIXDOBuM/s50-c-k-no/photo.jpg",
            "userId": "113711053352969780071"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "N = 100\n",
        "wins = {\n",
        "    'normal': 0,\n",
        "    'greedy': 0,\n",
        "    'random': 0\n",
        "}\n",
        "\n",
        "losses = {\n",
        "    'normal': 0,\n",
        "    'greedy': 0,\n",
        "    'random': 0\n",
        "}\n",
        "\n",
        "for n in range(N):\n",
        "  K = 5\n",
        "  action_space = ActionSpace(K=K, mu_range=[1, 10000], sd_max=25)\n",
        "  agent = Agent(K=K, eps=0.5)\n",
        "  greedy_agent = Agent(K=K, eps=0)\n",
        "  random_agent = Agent(K=K, eps=1)\n",
        "  \n",
        "  T = 1000\n",
        "  for _ in range(T):\n",
        "    agent.take_action(action_space)\n",
        "    greedy_agent.take_action(action_space)\n",
        "    random_agent.take_action(action_space)\n",
        "    \n",
        "  normal_value = agent.get_value()\n",
        "  greedy_value = greedy_agent.get_value()\n",
        "  random_value = random_agent.get_value()\n",
        "  \n",
        "  max_value = max(normal_value, greedy_value, random_value)\n",
        "  min_value = min(normal_value, greedy_value, random_value)\n",
        "  \n",
        "  if max_value == normal_value:\n",
        "    wins['normal'] += 1\n",
        "  elif max_value == greedy_value:\n",
        "    wins['greedy'] += 1\n",
        "  else:\n",
        "    wins['random'] += 1\n",
        "    \n",
        "  if min_value == normal_value:\n",
        "    losses['normal'] += 1\n",
        "  elif min_value == greedy_value:\n",
        "    losses['greedy'] += 1\n",
        "  else:\n",
        "    losses['random'] += 1\n",
        "print('wins', wins)\n",
        "print('losses', losses)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wins {'normal': 68, 'greedy': 32, 'random': 0}\n",
            "losses {'normal': 0, 'greedy': 50, 'random': 50}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnTQ2x-X8vhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that 68 out of the 100 times, the agent that balanced exploration and exploitation had the highest value. And notably, that agent never once had the lowest value. Also, it can be inferred from these results that the greedy agent, when it gets a lucky first pick, can get the highest value over time. However, if it chooses a poor first choice, then it can end up being worse than the other two agents.\n",
        "\n",
        "**In the end, we can conclude that some type of learning from previous actions, even if it is basic, is beneficial to achieving the long-term goal of maximizing value.**"
      ]
    }
  ]
}